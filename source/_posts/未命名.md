Spark的设计目的是高吞吐和批处理；它不能执行点度、点更新等处理，且数据不可变，要想做到数据共享，需要从外部加载数据源。

SnappyData扩展Spark目的是做个实时计算的数据平台：
增加可变性；
允许跨应用程序共享数据；
同时满足点查和多维聚合操作。

Druid存储有要求：时间戳、维度、metric。
Druid支持预聚合，通过roll-up将业务上最细粒度的维度做聚合。例如查询的范围是1分钟，那么写入时就把时间戳放到不同的窗口上，实时聚合。牺牲了明细数据，降低了存储容量。

Druid实时写入不支持exactly once语义的写入，批量写入支持。

Druid的查询原生支持json，但是做了sql的有限扩展。

Druid不支持join，都是提前打成宽表，将维度和结果放到一行里边。



